{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data\n",
    "From my database of 10,747,549 tweets obtained with keywords \"coronavirus\" or \"COVID-19\", I'm working with the first 200,000 to make exploratory data analysis quick and easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the data outputted to csv from the MySQL database is corrupted; \n",
    "# the `error_bad_lines` parameter skips over those rows.\n",
    "file = '../../../Documents/First200K_tweets.csv'\n",
    "df = pd.read_csv(file, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To each and every person celebrating this news...</td>\n",
       "      <td>Far From Home, WY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fusco-Jackson died a day before her test for c...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And yet asymptomati… https://t.co/dUlngepDQV\"</td>\n",
       "      <td>North Dakota, USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coronavirus is one thing. We clearly can’t be ...</td>\n",
       "      <td>Sacramento, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With @ParentMail down, this also failed to be ...</td>\n",
       "      <td>Balham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet           location\n",
       "0  To each and every person celebrating this news...  Far From Home, WY\n",
       "1  Fusco-Jackson died a day before her test for c...                NaN\n",
       "2      And yet asymptomati… https://t.co/dUlngepDQV\"  North Dakota, USA\n",
       "3  Coronavirus is one thing. We clearly can’t be ...     Sacramento, CA\n",
       "4  With @ParentMail down, this also failed to be ...             Balham"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199377"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 633 rows were not imported because of some kind of corruption in the data\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fusco-Jackson died a day before her test for coronavirus came back positive Saturday evening\"'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look at the full text of the second tweet\n",
    "df.iloc[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping rows\n",
    "Since my goal is to correlate the tweet's content with its location, I'm dropping every row with no location listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144545"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the original 200K tweets, we're down to 144,545\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing eliminates spelling differences due to capitalization variance\n",
    "df.location = df.location.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To each and every person celebrating this news...</td>\n",
       "      <td>far from home, wy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And yet asymptomati… https://t.co/dUlngepDQV\"</td>\n",
       "      <td>north dakota, usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coronavirus is one thing. We clearly can’t be ...</td>\n",
       "      <td>sacramento, ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With @ParentMail down, this also failed to be ...</td>\n",
       "      <td>balham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I don’t understand why famous people who “ fee...</td>\n",
       "      <td>south jersey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet           location\n",
       "0  To each and every person celebrating this news...  far from home, wy\n",
       "2      And yet asymptomati… https://t.co/dUlngepDQV\"  north dakota, usa\n",
       "3  Coronavirus is one thing. We clearly can’t be ...     sacramento, ca\n",
       "4  With @ParentMail down, this also failed to be ...             balham\n",
       "5  I don’t understand why famous people who “ fee...       south jersey"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas 1.0 allows Series to be converted to string datatype for faster processing\n",
    "df = df.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringDtype"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BREAKING NFL NEWS ⁦@CenTexBeat⁩  https://t.co/...</td>\n",
       "      <td>waco, tx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>https://t.co/c1JEnxeJ5X Austin Area Food Bank\\...</td>\n",
       "      <td>texas, usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>This tweet takes on a whole new level of “pres...</td>\n",
       "      <td>texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>@DonaldJTrumpJr The media? So Trump tv aka Fox...</td>\n",
       "      <td>texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Check it out y'all it's @TumaTime in the Obser...</td>\n",
       "      <td>tx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199152</th>\n",
       "      <td>+/- doubling every 2 days</td>\n",
       "      <td>k t boundary, tx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199182</th>\n",
       "      <td>Analysis | Trump’s eruption at an NBC reporter...</td>\n",
       "      <td>san antonio, tx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199273</th>\n",
       "      <td>Being quarantined while married gives you valu...</td>\n",
       "      <td>dallas, tx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199293</th>\n",
       "      <td>Mortality Rate of Coronavirus in US Slips to 1...</td>\n",
       "      <td>texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199367</th>\n",
       "      <td>I felt like i was reading someone’s essay the ...</td>\n",
       "      <td>satx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5629 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet          location\n",
       "7       BREAKING NFL NEWS ⁦@CenTexBeat⁩  https://t.co/...          waco, tx\n",
       "70      https://t.co/c1JEnxeJ5X Austin Area Food Bank\\...        texas, usa\n",
       "121     This tweet takes on a whole new level of “pres...             texas\n",
       "131     @DonaldJTrumpJr The media? So Trump tv aka Fox...             texas\n",
       "168     Check it out y'all it's @TumaTime in the Obser...                tx\n",
       "...                                                   ...               ...\n",
       "199152                          +/- doubling every 2 days  k t boundary, tx\n",
       "199182  Analysis | Trump’s eruption at an NBC reporter...   san antonio, tx\n",
       "199273  Being quarantined while married gives you valu...        dallas, tx\n",
       "199293  Mortality Rate of Coronavirus in US Slips to 1...             texas\n",
       "199367  I felt like i was reading someone’s essay the ...             satx \n",
       "\n",
       "[5629 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I was curious about how many of these 144,000 tweets claim a Texas location.\n",
    "# This cell outputs a dataframe of the tweets; its length is 5629 rows, a number I could\n",
    "# also obtain by wrapping the statement below in `len()`\n",
    "\n",
    "df[(df['location'].str.contains('texas'))|(df['location'].str.contains('tx'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Boolean methods, I can also get a count this way because False = 0 and True = 1\n",
    "df['location'].str.contains('texas').sum() + df['location'].str.contains('TX').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locations\n",
    "This will be the most time-consuming portion of data cleaning, and is something I'm working on elsewhere. The 39,662 unique locations below are actually many variant spellings and specificity of a smaller number of identical locations. Also, roughly 20% of people put a joke location like \"my kitchen\" as their current location. My goal is to condense location data to areas of different sizes, from large city to state/province to nation, when such information is available. \n",
    "\n",
    "For example, `df[199152]['location'] = 'k t boundary, tx'`. I'll re-classify that as Texas, USA. With `df[199273]['location'] = 'dallas, tx',` I can add additional information to classify it as Dallas, Texas, USA. For `df[199367]['location'] = 'satx'`, I'll have to decide how much work I want to put into decoding cities with common abbreviations such as San Antonio, Texas, USA, vs. just putting the state and nation, i.e., Texas, USA. In some cases, it may be useful to have city-level information when it's available. \n",
    "\n",
    "Additional exploratory analysis will be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39662"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique locations\n",
    "df.location.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Functions\n",
    "I'm using these so I can create extra columns in the dataframe and see if there's any correlation between these subsets of data and the location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_urls(string):\n",
    "    \"\"\"\n",
    "    From a string, returns text that is cleaned of any URLs starting in 'http' or 'https'\n",
    "\n",
    "    \"\"\"\n",
    "    wordlist = string.split()\n",
    "    text = ' '.join(word for word in wordlist if not 'http' in word)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And yet asymptomati… https://t.co/dUlngepDQV\"'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 'before'\n",
    "df['tweet'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and yet asymptomati…'"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After\n",
    "no_urls(df['tweet'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtags(string):\n",
    "    \"\"\"\n",
    "    From a string, returns any text that contains a hashtag\n",
    "\n",
    "    \"\"\"\n",
    "    wordlist = string.split()\n",
    "    text = ' '.join(word for word in wordlist if '#' in word)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def at_mention(string):\n",
    "    \"\"\"\n",
    "    From a string, returns any text that contains a @\n",
    "\n",
    "    \"\"\"\n",
    "    wordlist = string.split()\n",
    "    text = ' '.join(word for word in wordlist if '@' in word)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_tweet(string):\n",
    "    \"\"\"\n",
    "    Removes URLs, hashtags, and @ mentions in tweets\n",
    "    \"\"\"\n",
    "    wordlist = string.split()\n",
    "    text = ' '.join(word for word in wordlist if 'http' not in word and '@' not in word and '#' not in word)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `apply` function lets me apply my function to every cell in the Pandas column\n",
    "df['no_url'] = df['tweet'].apply(no_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtags'] = df['tweet'].apply(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['at_mention'] = df['tweet'].apply(at_mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned'] = df['tweet'].apply(cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>location</th>\n",
       "      <th>no_url</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>at_mention</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To each and every person celebrating this news...</td>\n",
       "      <td>far from home, wy</td>\n",
       "      <td>To each and every person celebrating this news...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>to each and every person celebrating this news...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And yet asymptomati… https://t.co/dUlngepDQV\"</td>\n",
       "      <td>north dakota, usa</td>\n",
       "      <td>And yet asymptomati…</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>and yet asymptomati…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coronavirus is one thing. We clearly can’t be ...</td>\n",
       "      <td>sacramento, ca</td>\n",
       "      <td>Coronavirus is one thing. We clearly can’t be ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>coronavirus is one thing. we clearly can’t be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With @ParentMail down, this also failed to be ...</td>\n",
       "      <td>balham</td>\n",
       "      <td>With @ParentMail down, this also failed to be ...</td>\n",
       "      <td></td>\n",
       "      <td>@parentmail</td>\n",
       "      <td>with down, this also failed to be sent out ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I don’t understand why famous people who “ fee...</td>\n",
       "      <td>south jersey</td>\n",
       "      <td>I don’t understand why famous people who “ fee...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>i don’t understand why famous people who “ fee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet           location  \\\n",
       "0  To each and every person celebrating this news...  far from home, wy   \n",
       "2      And yet asymptomati… https://t.co/dUlngepDQV\"  north dakota, usa   \n",
       "3  Coronavirus is one thing. We clearly can’t be ...     sacramento, ca   \n",
       "4  With @ParentMail down, this also failed to be ...             balham   \n",
       "5  I don’t understand why famous people who “ fee...       south jersey   \n",
       "\n",
       "                                              no_url hashtags   at_mention  \\\n",
       "0  To each and every person celebrating this news...                         \n",
       "2                               And yet asymptomati…                         \n",
       "3  Coronavirus is one thing. We clearly can’t be ...                         \n",
       "4  With @ParentMail down, this also failed to be ...           @parentmail   \n",
       "5  I don’t understand why famous people who “ fee...                         \n",
       "\n",
       "                                             cleaned  \n",
       "0  to each and every person celebrating this news...  \n",
       "2                               and yet asymptomati…  \n",
       "3  coronavirus is one thing. we clearly can’t be ...  \n",
       "4  with down, this also failed to be sent out ear...  \n",
       "5  i don’t understand why famous people who “ fee...  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet         143491\n",
       "location       39662\n",
       "no_url        131330\n",
       "hashtags       13319\n",
       "at_mention     21731\n",
       "cleaned       128825\n",
       "dtype: int64"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of unique hashtags and @ mentions is misleading; this is showing the unique ordering\n",
    "# and inclusion of each in any given row. NLTK tokenization will give me a real total count of each.\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and yet asymptomati…',\n",
       " 'coronavirus is one thing. we clearly can’t be closing down golf courses though.',\n",
       " 'with down, this also failed to be sent out earlier this afternoon (attachement to the letter)',\n",
       " 'i don’t understand why famous people who “ feel like they’re getting a cold “ can automatically get tested for coro…']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look at the first five cleaned up tweets\n",
    "# For the purposes of modeling, this is one of the datasets I'll try out.\n",
    "[item for item in df['cleaned'][1:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm joining the text with line returns so that the tweets will remain discrete.\n",
    "clean_text = '\\n'.join(tweet for tweet in df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11270314"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These cleaned up texts for only 144,000 tweets still generate more than 11 million characters.\n",
    "len(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the text file for use elsewhere\n",
    "with open('clean_text.txt', 'w') as outfile:\n",
    "    outfile.write(clean_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
